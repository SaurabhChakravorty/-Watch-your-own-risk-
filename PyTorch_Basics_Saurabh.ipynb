{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch_Basics_Saurabh.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOYHYSPH7rE50Hh3PN6qDFQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaurabhChakravorty/Watch-at-your-own-risk/blob/master/PyTorch_Basics_Saurabh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPG4ObuUxp3Y",
        "colab_type": "text"
      },
      "source": [
        "## Torch basics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kaNkJBTxavx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "28f93e4a-e6d4-478d-dab0-0175b040bca9"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "print(torch.rand(3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.8102, 0.3774, 0.7459])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv5Gqk3Rxd1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if torch.cuda.is_available():\n",
        " device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXr3q9Y5PwTd",
        "colab_type": "text"
      },
      "source": [
        "#### Importing pytorch_xla for TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-rJ6E7SPvZn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "outputId": "f732dd56-33b6-40d2-fb3f-2448bbd75d25"
      },
      "source": [
        "VERSION = \"20200325\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  4264  100  4264    0     0  65600      0 --:--:-- --:--:-- --:--:-- 65600\n",
            "Updating TPU and VM. This may take around 2 minutes.\n",
            "Updating TPU runtime to pytorch-dev20200325 ...\n",
            "Uninstalling torch-1.5.0a0+d6149a7:\n",
            "  Successfully uninstalled torch-1.5.0a0+d6149a7\n",
            "Uninstalling torchvision-0.6.0a0+3c254fb:\n",
            "  Successfully uninstalled torchvision-0.6.0a0+3c254fb\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\n",
            "\\ [1 files][ 83.4 MiB/ 83.4 MiB]                                                \n",
            "Operation completed over 1 objects/83.4 MiB.                                     \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\n",
            "\\ [1 files][114.5 MiB/114.5 MiB]                                                \n",
            "Operation completed over 1 objects/114.5 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  2.5 MiB/  2.5 MiB]                                                \n",
            "Operation completed over 1 objects/2.5 MiB.                                      \n",
            "Processing ./torch-nightly+20200325-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200325) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200325) (1.18.5)\n",
            "Done updating TPU runtime: <Response [200]>\n",
            "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.5.0a0+d6149a7\n",
            "Processing ./torch_xla-nightly+20200325-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "  Found existing installation: torch-xla 1.6+e788e5b\n",
            "    Uninstalling torch-xla-1.6+e788e5b:\n",
            "      Successfully uninstalled torch-xla-1.6+e788e5b\n",
            "Successfully installed torch-xla-1.6+e788e5b\n",
            "Processing ./torchvision-nightly+20200325-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (1.18.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (1.5.0a0+d6149a7)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (7.0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly+20200325) (0.16.0)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.6.0a0+3c254fb\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libomp5 is already the newest version (5.0.1-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 43 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umSDPG3lOoMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports pytorch\n",
        "import torch\n",
        "# imports the torch_xla package\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as x"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q36STJ_VRSPY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a49a4987-bc88-4bf7-9952-6fbd63332b76"
      },
      "source": [
        "# Creating a tensor on the second Cloud TPU core\n",
        "second_dev = x.xla_device(n=2, devkind='TPU')\n",
        "t2 = torch.zeros(3, 3, device = second_dev)\n",
        "print(t2)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]], device='xla:2')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU_6A-HbzKB5",
        "colab_type": "text"
      },
      "source": [
        "#### Some very basics torch computations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QMyfqrKxjNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Making n*n dimensionla pytorch tensor\n",
        "x = torch.empty(3,1,1)\n",
        "#print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9XYvIyYyF9B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7d598301-86c2-4eba-d080-19314ca1ff50"
      },
      "source": [
        "x = torch.ones(2,2,dtype=torch.float16)\n",
        "print(x.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOqczn51yqfQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c851d0e-4491-4aae-f13b-df1fcaeacbbc"
      },
      "source": [
        "x = torch.tensor([2.5,0.1])\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([2.5000, 0.1000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWQo0RyAy0bn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9d86fd85-d429-4444-a7c7-2ca74f7d8c73"
      },
      "source": [
        "x = torch.rand(2,2)\n",
        "y = torch.rand(2,2)\n",
        "z = x + y\n",
        "print(z)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.7209, 1.1255],\n",
            "        [0.5529, 0.9585]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbSktJ0tzFtm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5a930dc2-8ee7-44c2-dc33-060d0ede104c"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "a = torch.ones(5, device=device)\n",
        "print(a)\n",
        "# numpy doesnot supports GPU\n",
        "a = a.to(\"cpu\")\n",
        "b = a.numpy()\n",
        "print(b)\n",
        "# Underscore alters the original value in location\n",
        "a.add_(1)\n",
        "print(\"a :\",a)\n",
        "print(\"b :\",b)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1., 1., 1.], device='cuda:0')\n",
            "[1. 1. 1. 1. 1.]\n",
            "a : tensor([2., 2., 2., 2., 2.])\n",
            "b : [2. 2. 2. 2. 2.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tyTn6v3z55y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a4a17525-f7a3-4466-caac-667e787b1024"
      },
      "source": [
        "# adding torch to numpy \n",
        "a + b"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4., 4., 4., 4., 4.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOxnAZ9R4-R3",
        "colab_type": "text"
      },
      "source": [
        "### Computational graphs using AutoGrad\n",
        "\n",
        " One interesting property of pytorch is allows computational graph to be implemented using AutoGrad functionality\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhhCQA9k0NoH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "340ec7c5-67ef-4f58-e52b-0a2616595fa5"
      },
      "source": [
        "x = torch.rand(3, requires_grad=True,device=device)\n",
        "\n",
        "y = x + 2\n",
        "print(\"x:\",x)  # Input \n",
        "print(\"y:\",y)  # Forward\n",
        "\n",
        "z = y        \n",
        "#z = y.mean()\n",
        "print(z)\n",
        "#z = y.mean()\n",
        "print(z)\n",
        "# Backward does only for scalar\n",
        "# if not scalar we have to supply gradients\n",
        "v = torch.tensor([1.0,1.0,0.001],dtype=torch.float32,device=device)\n",
        "z.backward(v)\n",
        "print(\"z: \",z)\n",
        "#print(x.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x: tensor([0.6278, 0.4813, 0.8419], device='cuda:0', requires_grad=True)\n",
            "y: tensor([2.6278, 2.4813, 2.8419], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([2.6278, 2.4813, 2.8419], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([2.6278, 2.4813, 2.8419], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "z:  tensor([2.6278, 2.4813, 2.8419], device='cuda:0', grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7zr3ovz5h7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  x.requires_grad_(False)\n",
        "#  x.detach()\n",
        "# with torch.no_grad():\n",
        "#   y = x + 2\n",
        "#   print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-4uGrTWDdrP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e99f9c04-d202-49ab-e281-5ea7d7c25efa"
      },
      "source": [
        "weights = torch.ones(4,requires_grad=True)\n",
        "\n",
        "for epoch in range(3):\n",
        "    model_output = (weights*3).sum()\n",
        "\n",
        "    model_output.backward()\n",
        "\n",
        "    print(weights.grad)\n",
        "\n",
        "    weights.grad.zero_()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgcChlZCD6ZB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ccbe4e6c-25da-4048-9ccb-c2fd8f962150"
      },
      "source": [
        "x = torch.tensor(1.0)\n",
        "y = torch.tensor(2.0)\n",
        "\n",
        "w = torch.tensor(1.0, requires_grad=True)\n",
        "\n",
        "# forward pass and compute the loss\n",
        "y_hat = w * x\n",
        "loss = (y_hat - y) ** 2\n",
        "print(loss)\n",
        "\n",
        "\n",
        "# backward pass\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1., grad_fn=<PowBackward0>)\n",
            "tensor(-2.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y852Ebu1uStV",
        "colab_type": "text"
      },
      "source": [
        "### PyTorch GD + Backpropogation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kCUnC-qpDKS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "6920559e-3a79-4e01-f647-944cc1635933"
      },
      "source": [
        "X = torch.tensor([1,2,3,4],device=device)\n",
        "Y = torch.tensor([2,4,6,8],device=device)\n",
        "#print(X,Y)\n",
        "\n",
        "w = 0.0\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "    return w * x\n",
        "\n",
        "# Compute loss\n",
        "def loss(y,y_pred):\n",
        "    return ((y_pred - y) ** 2).mean()\n",
        "\n",
        "# gradient\n",
        "# MSE = 1 / N * (w*x - y)**2\n",
        "# dJ/ dW = 1 / N 2x (wx - y)\n",
        "\n",
        "def gradient(x,y,y_pred):\n",
        "    print(2*x , y_pred - y)\n",
        "    return ((2*x) * (y_pred - y)).mean()\n",
        "\n",
        "\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 10\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    # prediction = forward pss\n",
        "    y_pred = forward(X)\n",
        "\n",
        "    # loss\n",
        "    l = loss(Y,y_pred)\n",
        "\n",
        "    # gradients\n",
        "    dW = gradient(X,Y,y_pred)\n",
        "\n",
        "    # update weights\n",
        "    w -= learning_rate * dW\n",
        "\n",
        "    if epoch%1 == 0:\n",
        "     print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "print(f'Prediction after training: f(5) = {forward(5): .3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([2, 4, 6, 8], device='cuda:0') tensor([-2., -4., -6., -8.], device='cuda:0')\n",
            "epoch 1: w = 0.300, loss = 30.00000000\n",
            "tensor([2, 4, 6, 8], device='cuda:0') tensor([-1.7000, -3.4000, -5.1000, -6.8000], device='cuda:0')\n",
            "epoch 2: w = 0.555, loss = 21.67499924\n",
            "tensor([2, 4, 6, 8], device='cuda:0') tensor([-1.4450, -2.8900, -4.3350, -5.7800], device='cuda:0')\n",
            "epoch 3: w = 0.772, loss = 15.66018772\n",
            "tensor([2, 4, 6, 8], device='cuda:0') tensor([-1.2283, -2.4565, -3.6848, -4.9130], device='cuda:0')\n",
            "epoch 4: w = 0.956, loss = 11.31448650\n",
            "tensor([2, 4, 6, 8], device='cuda:0') tensor([-1.0440, -2.0880, -3.1320, -4.1761], device='cuda:0')\n",
            "epoch 5: w = 1.113, loss = 8.17471695\n",
            "tensor([2, 4, 6, 8], device='cuda:0') tensor([-0.8874, -1.7748, -2.6622, -3.5496], device='cuda:0')\n",
            "epoch 6: w = 1.246, loss = 5.90623379\n",
            "tensor([2, 4, 6, 8], device='cuda:0') tensor([-0.7543, -1.5086, -2.2629, -3.0172], device='cuda:0')\n",
            "epoch 7: w = 1.359, loss = 4.26725388\n",
            "tensor([2, 4, 6, 8], device='cuda:0') tensor([-0.6412, -1.2823, -1.9235, -2.5646], device='cuda:0')\n",
            "epoch 8: w = 1.455, loss = 3.08309102\n",
            "tensor([2, 4, 6, 8], device='cuda:0') tensor([-0.5450, -1.0900, -1.6349, -2.1799], device='cuda:0')\n",
            "epoch 9: w = 1.537, loss = 2.22753334\n",
            "tensor([2, 4, 6, 8], device='cuda:0') tensor([-0.4632, -0.9265, -1.3897, -1.8529], device='cuda:0')\n",
            "epoch 10: w = 1.606, loss = 1.60939264\n",
            "Prediction after training: f(5) =  8.031\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2vqpLsJlIXX",
        "colab_type": "text"
      },
      "source": [
        "### Training with model, loss and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o9HTmvuuqN_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "d44b11a8-d0a8-4d4d-b4ff-49cb82d4935e"
      },
      "source": [
        "\"\"\"\n",
        "# 1) Design model ( input, output size and forward pass)\n",
        "# 2) Reconstruct loss and optimizer\n",
        "# 3) Training loop\n",
        "#  - forward pass   : compute prediction\n",
        "#  - backward pass  : gradients\n",
        "#  - update weights \n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# 0) Training samples\n",
        "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32, device=device)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32, device=device)\n",
        "\n",
        "# 1) Design Model: Weights to optimize and forward function\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True, device=device)\n",
        "\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "    return w * x\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5).item():.3f}')\n",
        "\n",
        "# training\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD([w],lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    # prediction = forward pass\n",
        "    y_pred = forward(X)\n",
        "\n",
        "    # loss \n",
        "    l = loss(Y, y_pred)\n",
        "\n",
        "    # gradients = backward pass\n",
        "    l.backward()\n",
        "\n",
        "    # update weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # zero gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "     print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {forward(5): .3f}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 0.300, loss = 30.00000000\n",
            "epoch 11: w = 1.665, loss = 1.16278636\n",
            "epoch 21: w = 1.934, loss = 0.04506905\n",
            "epoch 31: w = 1.987, loss = 0.00174685\n",
            "epoch 41: w = 1.997, loss = 0.00006770\n",
            "epoch 51: w = 1.999, loss = 0.00000262\n",
            "epoch 61: w = 2.000, loss = 0.00000010\n",
            "epoch 71: w = 2.000, loss = 0.00000000\n",
            "epoch 81: w = 2.000, loss = 0.00000000\n",
            "epoch 91: w = 2.000, loss = 0.00000000\n",
            "Prediction after training: f(5) =  10.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJFnXiOJxSB4",
        "colab_type": "text"
      },
      "source": [
        "### Linear regression PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5U7mwhij0q0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "15d29a33-e15c-4992-d210-5f2a020d11a8"
      },
      "source": [
        "# linear regresssion \n",
        "# f = w.x + b\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 0) Training samples\n",
        "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
        "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
        "\n",
        "n_samples, n_features =  X.shape\n",
        "print(f'#samples: {n_samples}, #features: {n_features}')\n",
        "\n",
        "X_test = torch.tensor([5], dtype=torch.float32)\n",
        "\n",
        "# we can call this model with samples X\n",
        "model = nn.Linear(n_features, n_features)  # initialise the model with length of features\n",
        "\n",
        "\"\"\"\n",
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self,input_dim, output_dim):\n",
        "        super(LinearRegression,self)._init__()\n",
        "        self.lin = nn.LinearRegression(input_dim, output_dim)\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.lin(x)\n",
        "\n",
        "model = LinearRegression(n_samples, n_features)\n",
        "\n",
        "\"\"\"\n",
        "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
        "\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    # predict = forward pass with our model\n",
        "    y_predicted = model(X)\n",
        "\n",
        "    # loss\n",
        "    l = loss(Y, y_predicted)\n",
        "\n",
        "    # calculate gradients = backward pass\n",
        "    l.backward()\n",
        "\n",
        "    # update weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # zero the gradients after updating\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        [w, b] = model.parameters() # unpack parameters\n",
        "        print('epoch ', epoch+1, ': w = ', w[0][0].item(), ' loss = ', l)\n",
        "\n",
        "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#samples: 4, #features: 1\n",
            "Prediction before training: f(5) = -2.162\n",
            "epoch  1 : w =  0.057706475257873535  loss =  tensor(47.2372, grad_fn=<MeanBackward0>)\n",
            "epoch  11 : w =  1.6335623264312744  loss =  tensor(1.2281, grad_fn=<MeanBackward0>)\n",
            "epoch  21 : w =  1.8886327743530273  loss =  tensor(0.0374, grad_fn=<MeanBackward0>)\n",
            "epoch  31 : w =  1.9312092065811157  loss =  tensor(0.0062, grad_fn=<MeanBackward0>)\n",
            "epoch  41 : w =  1.9395604133605957  loss =  tensor(0.0051, grad_fn=<MeanBackward0>)\n",
            "epoch  51 : w =  1.9423621892929077  loss =  tensor(0.0048, grad_fn=<MeanBackward0>)\n",
            "epoch  61 : w =  1.9442282915115356  loss =  tensor(0.0045, grad_fn=<MeanBackward0>)\n",
            "epoch  71 : w =  1.945901870727539  loss =  tensor(0.0042, grad_fn=<MeanBackward0>)\n",
            "epoch  81 : w =  1.9475042819976807  loss =  tensor(0.0040, grad_fn=<MeanBackward0>)\n",
            "epoch  91 : w =  1.949055552482605  loss =  tensor(0.0038, grad_fn=<MeanBackward0>)\n",
            "Prediction after training: f(5) = 9.898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHUHjRhY3BNG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "d8190046-4d22-4f42-b7a7-08a24624e71d"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Prepare data\n",
        "X_numpy , y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=4)\n",
        "\n",
        "# cast to float tensor\n",
        "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
        "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
        "y = y.view(y.shape[0], 1)\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "# 1) Model\n",
        "# Linear model f = wx + b\n",
        "input_size = n_features\n",
        "output_size = 1\n",
        "model = nn.Linear(input_size, output_size)\n",
        "\n",
        "\n",
        "# 2) Loss and optimizer\n",
        "learning_rate = 0.01\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "# 3) Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    # forward pass and loss\n",
        "    y_predicted = model(X)\n",
        "    loss = criterion(y_predicted, y)\n",
        "    \n",
        "    # backward pass and update\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # zero grad before new step\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
        "\n",
        "# Plot\n",
        "predicted = model(X).detach().numpy()\n",
        "plt.plot(X_numpy, y_numpy, 'ro')\n",
        "plt.plot(X_numpy, predicted, 'b')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 10, loss = 4141.1792\n",
            "epoch: 20, loss = 2915.7300\n",
            "epoch: 30, loss = 2080.6138\n",
            "epoch: 40, loss = 1511.3746\n",
            "epoch: 50, loss = 1123.2816\n",
            "epoch: 60, loss = 858.6342\n",
            "epoch: 70, loss = 678.1289\n",
            "epoch: 80, loss = 554.9886\n",
            "epoch: 90, loss = 470.9661\n",
            "epoch: 100, loss = 413.6236\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGlzZiLrPCy4",
        "colab_type": "text"
      },
      "source": [
        "### Logistic regression PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rhbf8-0uHuKa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "fb83cdaf-5412-4086-807f-ccce88395479"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 0) Prepare data\n",
        "bc = datasets.load_breast_cancer()\n",
        "X, y = bc.data, bc.target\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "\n",
        "\n",
        "# scale\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test  = sc.transform(X_test)\n",
        "\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test  = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test  = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "# 1) Model\n",
        "# Linear model f = wx + b , sigmoid at the end\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, n_input_features):\n",
        "        super(Model, self).__init__()\n",
        "        self.linear = nn.Linear(n_input_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y_pred = torch.sigmoid(self.linear(x))\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "model = Model(n_features)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# Using TPU\n",
        "dev = x.xla_device()\n",
        "second_dev = x.xla_device(n=2, devkind='TPU')\n",
        "\n",
        "X_train = X_train.to(second_dev)\n",
        "y_train = y_train.to(second_dev)\n",
        "X_test = X_test.to(second_dev)\n",
        "y_test = y_test.to(second_dev)\n",
        "model = model.to(second_dev)\n",
        "\"\"\"\n",
        "# 2) Loss and optimizer\n",
        "num_epochs = 100\n",
        "learning_rate = 0.01\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 3) Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass and loss\n",
        "    y_pred = model.forward(X_train)\n",
        "    y_pred = torch.flatten(y_pred)\n",
        "    loss = criterion(y_pred, y_train)\n",
        "\n",
        "    # Backward pass and update\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # zero grad before new step\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
        "\n",
        "\n",
        "\n",
        "y_predicted = model(X_test)\n",
        "y_predicted_cls = y_predicted.round()\n",
        "acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
        "print(f'accuracy: {acc.item():.4f}')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 10, loss = 0.4118\n",
            "epoch: 20, loss = 0.3608\n",
            "epoch: 30, loss = 0.3248\n",
            "epoch: 40, loss = 0.2979\n",
            "epoch: 50, loss = 0.2769\n",
            "epoch: 60, loss = 0.2600\n",
            "epoch: 70, loss = 0.2459\n",
            "epoch: 80, loss = 0.2340\n",
            "epoch: 90, loss = 0.2238\n",
            "epoch: 100, loss = 0.2149\n",
            "accuracy: 60.1579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbxppHLMYFu3",
        "colab_type": "text"
      },
      "source": [
        "### Binary classification using NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ca0XAEsCR2K1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "5ba5c7e5-64ae-4a09-c47a-9acb512b2766"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Using TPU\n",
        "dev = x.xla_device()\n",
        "\n",
        "# Option 1 (create nn modules)\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self,input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu    = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
        "        self.softmax = nn.LogSoftmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.linear2(out)\n",
        "        out = self.softmax(out)\n",
        "        return out\n",
        "\n",
        "input_size = 30\n",
        "model = NeuralNet(input_size,40,1)\n",
        "\n",
        "\n",
        "# Loss and optimizer\n",
        "num_epochs = 100\n",
        "learning_rate = 0.01\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "# Device configuration : Adding to GPU devices\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "X_train = X_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "X_test = X_test.to(device)\n",
        "y_test = y_test.to(device)\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass and loss\n",
        "    y_pred = model.forward(X_train)\n",
        "    y_pred = torch.flatten(y_pred)\n",
        "    loss = criterion(y_pred, y_train)\n",
        "\n",
        "    # Backward pass and update\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # zero grad before new step\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
        "\n",
        "\n",
        "y_predicted = model(X_test)\n",
        "y_predicted_cls = y_predicted.round()\n",
        "acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
        "print(f'accuracy: {acc.item():.4f}')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 10, loss = 63.2967\n",
            "epoch: 20, loss = 63.2967\n",
            "epoch: 30, loss = 63.2967\n",
            "epoch: 40, loss = 63.2967\n",
            "epoch: 50, loss = 63.2967\n",
            "epoch: 60, loss = 63.2967\n",
            "epoch: 70, loss = 63.2967\n",
            "epoch: 80, loss = 63.2967\n",
            "epoch: 90, loss = 63.2967\n",
            "epoch: 100, loss = 63.2967\n",
            "accuracy: 45.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCBbCwi0MuQm",
        "colab_type": "text"
      },
      "source": [
        "### Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQo-eIDyQ_GY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = retina\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import helper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn9jO1EGM9_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = Cat_Dog_data\n",
        "#Applying Transformation\n",
        "train_transforms = transforms.Compose([\n",
        "                                transforms.RandomRotation(30),\n",
        "                                transforms.RandomResizedCrop(224),\n",
        "                                transforms.RandomHorizontalFlip(),\n",
        "                                transforms.ToTensor()])\n",
        "test_transforms = transforms.Compose([transforms.Resize(255),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor()])\n",
        "train_data = datasets.ImageFolder(data_dir + /train,  \n",
        "                                    transform=train_transforms)                                       \n",
        "test_data = datasets.ImageFolder(data_dir + /test, \n",
        "                                    transform=test_transforms)\n",
        "\n",
        "#Data Loading\n",
        "trainloader = torch.utils.data.DataLoader(train_data,\n",
        "                                                   batch_size=32)\n",
        "testloader = torch.utils.data.DataLoader(test_data, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dng7XZdpNOFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images, labels = next(iter(dataloader))\n",
        "helper.imshow(images[0], normalize=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUUQ1EUWdkaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}