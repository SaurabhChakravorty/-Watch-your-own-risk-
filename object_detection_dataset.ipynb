{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "object_detection_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8829ce7544294e1d8e302a3d3dba3275": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5d85f78480a741e0ae0b235603e9ddc5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_34563681647f4dc0afa7274db4dca5fa",
              "IPY_MODEL_e2b87a84d7484ffd860d04333265ef0a"
            ]
          }
        },
        "5d85f78480a741e0ae0b235603e9ddc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "34563681647f4dc0afa7274db4dca5fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_32587bb55064471cb9b03d5aeeb75a28",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_61dcfbe9679e46c19c4f92b2ae6aac59"
          }
        },
        "e2b87a84d7484ffd860d04333265ef0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e9f3f9b1896e47b198c6eb3717d82ec9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 460038144/? [01:50&lt;00:00, 5304360.46it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b1a35371974941c6a9f8b3a9f04b0f9f"
          }
        },
        "32587bb55064471cb9b03d5aeeb75a28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "61dcfbe9679e46c19c4f92b2ae6aac59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e9f3f9b1896e47b198c6eb3717d82ec9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b1a35371974941c6a9f8b3a9f04b0f9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaurabhChakravorty/Watch-at-your-own-risk/blob/master/object_detection_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BRIqwJUr2HuN"
      },
      "source": [
        "# Single-Stage Object Detector\n",
        "In this exercise you will implement a **single-stage** object detector, based on YOLO ([v1](https://arxiv.org/pdf/1506.02640.pdf) and [v2](https://arxiv.org/pdf/1612.08242.pdf)) and use it to train a model that can detect objects on novel images. We will also evaluate the detection accuracy using the classic metric mean Average Precision ([mAP](https://github.com/Cartucho/mAP)). In Part II of A5, you will implement a **two-stage** object detector, based on [Faster R-CNN](https://arxiv.org/pdf/1506.01497.pdf). The main difference between the two is that single-stage detectors perform region proposal and classification simultaneously while two-stage detectors have them decoupled. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LfBk3NtRgqaV"
      },
      "source": [
        "# Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ubB_0e-UAOVK"
      },
      "source": [
        "## Install starter code\n",
        "We will continue using the utility functions that we've used for previous assignments: [`coutils` package](https://github.com/deepvision-class/starter-code). Run this cell to download and install it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ASkY27ZtA7Is",
        "outputId": "2a33eccc-a7fa-449a-c2d6-ef1edf972fee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "!pip install git+https://github.com/deepvision-class/starter-code"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/deepvision-class/starter-code\n",
            "  Cloning https://github.com/deepvision-class/starter-code to /tmp/pip-req-build-0697d0ar\n",
            "  Running command git clone -q https://github.com/deepvision-class/starter-code /tmp/pip-req-build-0697d0ar\n",
            "Requirement already satisfied: pydrive in /usr/local/lib/python3.6/dist-packages (from Colab-Utils==0.1.dev0) (1.3.1)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pydrive->Colab-Utils==0.1.dev0) (4.1.3)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from pydrive->Colab-Utils==0.1.dev0) (1.7.12)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from pydrive->Colab-Utils==0.1.dev0) (3.13)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive->Colab-Utils==0.1.dev0) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive->Colab-Utils==0.1.dev0) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive->Colab-Utils==0.1.dev0) (4.0)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive->Colab-Utils==0.1.dev0) (0.17.4)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive->Colab-Utils==0.1.dev0) (1.12.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive->Colab-Utils==0.1.dev0) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive->Colab-Utils==0.1.dev0) (1.7.2)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive->Colab-Utils==0.1.dev0) (0.0.3)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->pydrive->Colab-Utils==0.1.dev0) (47.1.1)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->pydrive->Colab-Utils==0.1.dev0) (3.1.1)\n",
            "Building wheels for collected packages: Colab-Utils\n",
            "  Building wheel for Colab-Utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Colab-Utils: filename=Colab_Utils-0.1.dev0-cp36-none-any.whl size=10323 sha256=d891ff395c110acd818b9f08c7a4e958800a7730528b4d335a8cb08b339e7ef8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-b_hwexvc/wheels/63/d1/27/a208931527abb98d326d00209f46c80c9d745851d6a1defd10\n",
            "Successfully built Colab-Utils\n",
            "Installing collected packages: Colab-Utils\n",
            "Successfully installed Colab-Utils-0.1.dev0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MzqbYcKdz6ew"
      },
      "source": [
        "## Setup code\n",
        "Run some setup code for this notebook: Import some useful packages and increase the default figure size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HzRdJ3uhe1CR",
        "tags": [
          "pdf-ignore"
        ],
        "outputId": "24eb823f-550d-4d16-96df-2bf5736597cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import coutils\n",
        "from coutils import extract_drive_file_id, register_colab_notebooks, \\\n",
        "                    fix_random_seed, rel_error\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import copy\n",
        "import time\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# for plotting\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# data type and device for torch.tensor\n",
        "to_float = {'dtype': torch.float, 'device': 'cpu'}\n",
        "to_float_cuda = {'dtype': torch.float, 'device': 'cuda'}\n",
        "to_double = {'dtype': torch.double, 'device': 'cpu'}\n",
        "to_double_cuda = {'dtype': torch.double, 'device': 'cuda'}\n",
        "to_long = {'dtype': torch.long, 'device': 'cpu'}\n",
        "to_long_cuda = {'dtype': torch.long, 'device': 'cuda'}\n",
        "\n",
        "# for mAP evaluation\n",
        "!rm -rf mAP\n",
        "!git clone https://github.com/Cartucho/mAP.git\n",
        "!rm -rf mAP/input/*"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'mAP'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 908 (delta 13), reused 18 (delta 5), pack-reused 880\u001b[K\n",
            "Receiving objects: 100% (908/908), 14.72 MiB | 8.09 MiB/s, done.\n",
            "Resolving deltas: 100% (317/317), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OvUDZWGU3VLV"
      },
      "source": [
        "We will use GPUs to accelerate our computation in this notebook. Run the following to make sure GPUs are enabled:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RrAX9FOLpr9k",
        "outputId": "8c07995a-4447-429b-adb7-9d551a7df12e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if torch.cuda.is_available:\n",
        "  print('Good to go!')\n",
        "else:\n",
        "  print('Please set GPU via Edit -> Notebook Settings.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Good to go!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MjJ3uyYBg3Lw"
      },
      "source": [
        "## Load PASCAL VOC 2007 data\n",
        "During the majority of our homework assignments so far, we have used the CIFAR-10 dataset for image classification tasks.\n",
        "\n",
        "We will need to use a new dataset for object detection. In order to train and evaluate object detection models, we need a dataset where each image is annotated with a *set* of *bounding boxes*, where each box gives the category label and spatial extent of some object in the image.\n",
        "\n",
        "We will use the [PASCAL VOC 2007](http://host.robots.ox.ac.uk/pascal/VOC/) dataset, which provides annotations of this form. PASCAL VOC ran a series of yearly computer vision competitions from 2005 to 2012, predating the ImageNet challenge which we have discussed in class.\n",
        "\n",
        "The data from the 2007 challenge used to be one of the most popular datasets for evaluating object detection. It is much smaller than more recent object detection datasets such as [COCO](http://cocodataset.org/#home), and thus easier to manage in an homework assignment.\n",
        "\n",
        "The following function will download the PASCAL VOC 2007 dataset and return it as a PyTorch Dataset object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fmD9Qrs2g7fI",
        "colab": {}
      },
      "source": [
        "def get_pascal_voc2007_data(image_root, split='train'):\n",
        "  \"\"\"\n",
        "  Use torchvision.datasets\n",
        "  https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.VOCDetection\n",
        "  \"\"\"\n",
        "  from torchvision import datasets\n",
        "\n",
        "  train_dataset = datasets.VOCDetection(image_root, year='2007', image_set=split,\n",
        "                                    download=True)\n",
        "  \n",
        "  return train_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XXc_Hw3JhVxw"
      },
      "source": [
        "Run the following cell to download the training and validation sets for the PASCAL VOC 2007 dataset:\n",
        "\n",
        "The [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) objects returned from the above function returns annotations for each image as a nested set of dictionary objects:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MmEP5KQJzk0d",
        "outputId": "e472d545-8090-424a-dc15-58b4ba2aa87e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865,
          "referenced_widgets": [
            "8829ce7544294e1d8e302a3d3dba3275",
            "5d85f78480a741e0ae0b235603e9ddc5",
            "34563681647f4dc0afa7274db4dca5fa",
            "e2b87a84d7484ffd860d04333265ef0a",
            "32587bb55064471cb9b03d5aeeb75a28",
            "61dcfbe9679e46c19c4f92b2ae6aac59",
            "e9f3f9b1896e47b198c6eb3717d82ec9",
            "b1a35371974941c6a9f8b3a9f04b0f9f"
          ]
        }
      },
      "source": [
        "# uncomment below to use the mirror link if the original link is broken\n",
        "# !wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n",
        "train_dataset = get_pascal_voc2007_data('/content', 'train')\n",
        "val_dataset = get_pascal_voc2007_data('/content', 'val')\n",
        "\n",
        "# an example on the raw annotation\n",
        "import json\n",
        "print(json.dumps(train_dataset[1][1]['annotation'], indent=2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar to /content/VOCtrainval_06-Nov-2007.tar\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8829ce7544294e1d8e302a3d3dba3275",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: /content/VOCtrainval_06-Nov-2007.tar\n",
            "{\n",
            "  \"folder\": \"VOC2007\",\n",
            "  \"filename\": \"000017.jpg\",\n",
            "  \"source\": {\n",
            "    \"database\": \"The VOC2007 Database\",\n",
            "    \"annotation\": \"PASCAL VOC2007\",\n",
            "    \"image\": \"flickr\",\n",
            "    \"flickrid\": \"228217974\"\n",
            "  },\n",
            "  \"owner\": {\n",
            "    \"flickrid\": \"genewolf\",\n",
            "    \"name\": \"whiskey kitten\"\n",
            "  },\n",
            "  \"size\": {\n",
            "    \"width\": \"480\",\n",
            "    \"height\": \"364\",\n",
            "    \"depth\": \"3\"\n",
            "  },\n",
            "  \"segmented\": \"0\",\n",
            "  \"object\": [\n",
            "    {\n",
            "      \"name\": \"person\",\n",
            "      \"pose\": \"Left\",\n",
            "      \"truncated\": \"0\",\n",
            "      \"difficult\": \"0\",\n",
            "      \"bndbox\": {\n",
            "        \"xmin\": \"185\",\n",
            "        \"ymin\": \"62\",\n",
            "        \"xmax\": \"279\",\n",
            "        \"ymax\": \"199\"\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"horse\",\n",
            "      \"pose\": \"Left\",\n",
            "      \"truncated\": \"0\",\n",
            "      \"difficult\": \"0\",\n",
            "      \"bndbox\": {\n",
            "        \"xmin\": \"90\",\n",
            "        \"ymin\": \"78\",\n",
            "        \"xmax\": \"403\",\n",
            "        \"ymax\": \"336\"\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J5MjBX9bkBtA"
      },
      "source": [
        "In order to use these annotations to train our model, we need to convert this nested dictionary data structure into a set of PyTorch tensors.\n",
        "\n",
        "We also need to preprocess the image, converting it to a PyTorch tensor and resizing it to 224x224. Real object detection systems typically work with much higher-resolution images, but we will use a low resolution for computational efficiency in this assignment.\n",
        "\n",
        "We also want to train our models using minibatches of data, so we need to group the annotations from several images into minibatches.\n",
        "\n",
        "We perform both of these functions by using a customized PyTorch [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) object, which we have written for you:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OfwTGpZn1L5U",
        "colab": {}
      },
      "source": [
        "def pascal_voc2007_loader(dataset, batch_size, num_workers=0):\n",
        "  \"\"\"\n",
        "  Data loader for Pascal VOC 2007.\n",
        "  https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
        "  \"\"\"\n",
        "  from torch.utils.data import DataLoader\n",
        "  # turn off shuffle so we can index the original image\n",
        "  train_loader = DataLoader(dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=False, pin_memory=True,\n",
        "                            num_workers=num_workers,\n",
        "                            collate_fn=voc_collate_fn)\n",
        "  return train_loader\n",
        "\n",
        "\n",
        "class_to_idx = {'aeroplane':0, 'bicycle':1, 'bird':2, 'boat':3, 'bottle':4,\n",
        "                'bus':5, 'car':6, 'cat':7, 'chair':8, 'cow':9, 'diningtable':10,\n",
        "                'dog':11, 'horse':12, 'motorbike':13, 'person':14, 'pottedplant':15,\n",
        "                'sheep':16, 'sofa':17, 'train':18, 'tvmonitor':19\n",
        "}\n",
        "idx_to_class = {i:c for c, i in class_to_idx.items()}\n",
        "\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "def voc_collate_fn(batch_lst, reshape_size=224):\n",
        "    preprocess = transforms.Compose([\n",
        "      transforms.Resize((reshape_size, reshape_size)),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "      ])\n",
        "    \n",
        "    batch_size = len(batch_lst)\n",
        "    \n",
        "    img_batch = torch.zeros(batch_size, 3, reshape_size, reshape_size)\n",
        "    \n",
        "    max_num_box = max(len(batch_lst[i][1]['annotation']['object']) \\\n",
        "                      for i in range(batch_size))\n",
        "\n",
        "    box_batch = torch.Tensor(batch_size, max_num_box, 5).fill_(-1.)\n",
        "    w_list = []\n",
        "    h_list = []\n",
        "    img_id_list = []\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "      img, ann = batch_lst[i]\n",
        "      w_list.append(img.size[0]) # image width\n",
        "      h_list.append(img.size[1]) # image height\n",
        "      img_id_list.append(ann['annotation']['filename'])\n",
        "      img_batch[i] = preprocess(img)\n",
        "      all_bbox = ann['annotation']['object']\n",
        "      if type(all_bbox) == dict: # inconsistency in the annotation file\n",
        "        all_bbox = [all_bbox]\n",
        "      for bbox_idx, one_bbox in enumerate(all_bbox):\n",
        "        bbox = one_bbox['bndbox']\n",
        "        obj_cls = one_bbox['name']\n",
        "        box_batch[i][bbox_idx] = torch.Tensor([float(bbox['xmin']), float(bbox['ymin']),\n",
        "          float(bbox['xmax']), float(bbox['ymax']), class_to_idx[obj_cls]])\n",
        "    \n",
        "    h_batch = torch.tensor(h_list)\n",
        "    w_batch = torch.tensor(w_list)\n",
        "\n",
        "    return img_batch, box_batch, w_batch, h_batch, img_id_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0ad8hHvAlGdA"
      },
      "source": [
        "Training with the entire PASCAL VOC will be too computationally expensive for this homework assignment, so we can subsample the dataset by wrapping each `Dataset` object in a [`Subset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset) object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XL-7Em_A1kdS",
        "colab": {}
      },
      "source": [
        "train_dataset = torch.utils.data.Subset(train_dataset, torch.arange(0, 2500)) # use 2500 samples for training\n",
        "train_loader = pascal_voc2007_loader(train_dataset, 1)\n",
        "val_loader = pascal_voc2007_loader(val_dataset, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOUTOGZnZzTX",
        "colab_type": "text"
      },
      "source": [
        "#You will be iterating through train_loader and val_loader to get training and validation dataset.\n",
        "\n",
        "- Iterate through the train_loader and val_loader\n",
        "- Save resized image and bounding box related information in appropriate form (dictionary, text file, JSON file or any other format of your choice)\n",
        "- We will be using this dataset for training of Faster R-CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eKsPwW7mTTa",
        "colab_type": "code",
        "outputId": "c54b5def-4a3e-497b-977b-81def8cbeb19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "for i, data_sample in enumerate(train_loader):\n",
        "  print(data_sample[0].shape) # This prints size of resized image which should be 1x3x224x224\n",
        "  print(data_sample[1])       # This prints tensor of shape 1x(number of objects in image)x5. \n",
        "                              # These 5 values indicate indices of BB and it's class  (x_min, y_min, x_max, y_max, cls)\n",
        "  print(data_sample[2], data_sample[3])  # This line prints actual size of image. This should be used in scaling and shifting\n",
        "                                         # values of bounding boxes\n",
        "  print(data_sample[4])       # This prints filename in PASCAL VOC dataset\n",
        "  print('-'*20)\n",
        "  if i == 1 : break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 224, 224])\n",
            "tensor([[[156.,  97., 351., 270.,   6.]]])\n",
            "tensor([500]) tensor([333])\n",
            "['000012.jpg']\n",
            "--------------------\n",
            "torch.Size([1, 3, 224, 224])\n",
            "tensor([[[185.,  62., 279., 199.,  14.],\n",
            "         [ 90.,  78., 403., 336.,  12.]]])\n",
            "tensor([480]) tensor([364])\n",
            "['000017.jpg']\n",
            "--------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCrN0ApMpEXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noCj7ACgAdy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}